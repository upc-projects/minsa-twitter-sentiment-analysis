{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import unidecode\n",
    "from utils import preprocess_text, remove_stopwords, lower_token\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from numpy import array, asarray, zeros\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rodrigo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rodrigo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we need to download nltk stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0        Salgo de #VeoTV , que día más largoooooo...      NONE\n",
       "1  @PauladeLasHeras No te libraras de ayudar me/n...       NEU\n",
       "2                          @marodriguezb Gracias MAR         P\n",
       "3  Off pensando en el regalito Sinde, la que se v...         N\n",
       "4  Conozco a alguien q es adicto al drama! Ja ja ...         P"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"corpus/corpus.csv\", names=['text','sentiment'], header=0)\n",
    "data2 = pd.read_csv(\"corpus/2clases_es_generaltassisol_pub.csv\", names=['text','sentiment'], header=0)\n",
    "data3 = pd.read_csv(\"corpus/data_def.csv\", names=['text','sentiment'], header=0)\n",
    "\n",
    "#Join all dataframes in a single dataframe\n",
    "data = pd.concat([data,data3,data2])\n",
    "\n",
    "#Check the data is correctly readed\n",
    "print(\"Dataframe data\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24882, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we check the shape\n",
    "print(\"Dataframe shape\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe sentiment count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "neg         7807\n",
       "pos         5393\n",
       "P           3531\n",
       "N           2865\n",
       "NONE        2664\n",
       "NEU         1123\n",
       "negativo     869\n",
       "neutro       494\n",
       "positivo     136\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we check the sentiment count\n",
    "print(\"Dataframe sentiment count\")\n",
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>Salgo de VeoTV que día más largoooooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>NEU</td>\n",
       "      <td>No te libraras de ayudar me nos Besos gracias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>P</td>\n",
       "      <td>Gracias MAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>N</td>\n",
       "      <td>Off pensando en el regalito Sinde la que se va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>P</td>\n",
       "      <td>Conozco alguien es adicto al drama Ja ja ja te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  \\\n",
       "0        Salgo de #VeoTV , que día más largoooooo...      NONE   \n",
       "1  @PauladeLasHeras No te libraras de ayudar me/n...       NEU   \n",
       "2                          @marodriguezb Gracias MAR         P   \n",
       "3  Off pensando en el regalito Sinde, la que se v...         N   \n",
       "4  Conozco a alguien q es adicto al drama! Ja ja ...         P   \n",
       "\n",
       "                                          text_clean  \n",
       "0             Salgo de VeoTV que día más largoooooo   \n",
       "1      No te libraras de ayudar me nos Besos gracias  \n",
       "2                                        Gracias MAR  \n",
       "3  Off pensando en el regalito Sinde la que se va...  \n",
       "4  Conozco alguien es adicto al drama Ja ja ja te...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre process tweets and save them as a new column in the dataframe\n",
    "data['text_clean'] = data['text'].apply(lambda x: preprocess_text(str(x)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform sentences into tokens\n",
    "tokens = [word_tokenize(sen) for sen in data.text_clean]\n",
    "\n",
    "#Put all the words to lowercase\n",
    "lower_tokens = [lower_token(token) for token in tokens]\n",
    "\n",
    "#Import spanish stopwords\n",
    "stoplist = stopwords.words('spanish')\n",
    "\n",
    "#Remove stopwords from sentences for better process\n",
    "filtered_words = [remove_stopwords(sen, stoplist) for sen in lower_tokens]\n",
    "\n",
    "#Update processed text from dataframe with the new filtered sentences\n",
    "data['text_clean'] = [' '.join(sen) for sen in filtered_words]\n",
    "#Create a new column that will have the same words but as tokens\n",
    "data['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform sentiment label to three columns in dataset for three outputs\n",
    "pos = []\n",
    "neg = []\n",
    "neu = []\n",
    "\n",
    "for sent in data.sentiment:\n",
    "    if sent == 'P' or sent=='pos' or sent=='positivo':\n",
    "        neu.append(0)\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "    elif sent == 'N' or sent=='neg' or sent=='negativo':\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "        neu.append(0)\n",
    "    else:\n",
    "        neu.append(1)\n",
    "        pos.append(0)\n",
    "        neg.append(0)\n",
    "        \n",
    "data['Pos'] = pos\n",
    "data['Neg'] = neg\n",
    "data['Neu'] = neu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neu</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>salgo veotv día largoooooo</td>\n",
       "      <td>[salgo, veotv, día, largoooooo]</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>libraras ayudar besos gracias</td>\n",
       "      <td>[libraras, ayudar, besos, gracias]</td>\n",
       "      <td>NEU</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gracias mar</td>\n",
       "      <td>[gracias, mar]</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>off pensando regalito sinde va sgae van corrup...</td>\n",
       "      <td>[off, pensando, regalito, sinde, va, sgae, van...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conozco alguien adicto drama ja ja ja suena</td>\n",
       "      <td>[conozco, alguien, adicto, drama, ja, ja, ja, ...</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  \\\n",
       "0                         salgo veotv día largoooooo   \n",
       "1                      libraras ayudar besos gracias   \n",
       "2                                        gracias mar   \n",
       "3  off pensando regalito sinde va sgae van corrup...   \n",
       "4        conozco alguien adicto drama ja ja ja suena   \n",
       "\n",
       "                                              tokens sentiment  Pos  Neu  Neg  \n",
       "0                    [salgo, veotv, día, largoooooo]      NONE    0    1    0  \n",
       "1                 [libraras, ayudar, besos, gracias]       NEU    0    1    0  \n",
       "2                                     [gracias, mar]         P    1    0    0  \n",
       "3  [off, pensando, regalito, sinde, va, sgae, van...         N    0    0    1  \n",
       "4  [conozco, alguien, adicto, drama, ja, ja, ja, ...         P    1    0    0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Redeclare dataframe with selected columns\n",
    "data = data[['text_clean', 'tokens', 'sentiment', 'Pos', 'Neu', 'Neg']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data for test and training \n",
    "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152518 words total, with a vocabulary size of 30260\n",
      "Max sentence length is 39\n"
     ]
    }
   ],
   "source": [
    "#Get total words in the train dataframe\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "\n",
    "#Get all the sentence lengths from train dataframe\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "\n",
    "#Get all the words without duplicates in the train dataframe\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "\n",
    "print(\"%s words, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16770 words total, with a vocabulary size of 7890\n",
      "Max sentence length is 30\n"
     ]
    }
   ],
   "source": [
    "#Get total words in the test dataframe\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "\n",
    "#Get all the sentence lengths from test dataframe\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "\n",
    "#Get all the words without duplicates in the test dataframe\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "\n",
    "print(\"%s words, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30260 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Now we start using tokenizer for sentences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50  #Max length that a sentence should have\n",
    "EMBEDDING_DIM = 300   #Dimension of embedding (the same as the dimension of glove embeddings)\n",
    "\n",
    "#Declare Tokenizer\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "\n",
    "#Fit tokenizer with training data\n",
    "tokenizer.fit_on_texts(data_train[\"text_clean\"].tolist())\n",
    "\n",
    "#Transform sentences from both datasets into sequences with tokenizer\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"text_clean\"].tolist())\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"text_clean\"].tolist())\n",
    "\n",
    "#Pad the sequences adding 0s to reach the max sequence length\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30261, 300)\n"
     ]
    }
   ],
   "source": [
    "#load glove embeddings\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "#Open glove file\n",
    "glove_file = open('glove/glove-sbwc.i25.vec', encoding=\"utf8\")\n",
    "\n",
    "#Iterate all lines in glove file \n",
    "for line in glove_file:\n",
    "    #Split words\n",
    "    records = line.split()\n",
    "    \n",
    "    #The first line should not be considered\n",
    "    if len(records) == 2:\n",
    "        continue\n",
    "        \n",
    "    #Save data in the dictionary\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "#Close glove file\n",
    "glove_file.close()\n",
    "\n",
    "#Create an array with the glove dimension of the embeddings and total unique tokens\n",
    "train_embedding_weights = zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "#Save embedding weights using weights from glove if has the word, otherwise use a random array with the same dimension\n",
    "for word, index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = embeddings_dictionary[word] if word in embeddings_dictionary else np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define the model\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    #Create the embedding layer of the model\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    #Create the model as a sequential model using keras\n",
    "    model = Sequential()\n",
    "    #Add the embedding layer to the model\n",
    "    model.add(embedding_layer)\n",
    "    #Add a convolutional layer of one dimension and 128 filters with tanh activation\n",
    "    model.add(Conv1D(128, 10, activation='tanh'))\n",
    "    #Add a global pooling layer\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    #Add a dense layer with 3 outputs using softmax activation, as we have 3 possible answers\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    #Compile model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the label names of the answers\n",
    "label_names = ['Pos', 'Neu', 'Neg']\n",
    "#Set training data to fit model\n",
    "y_train = data_train[label_names].values\n",
    "x_train = train_cnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 50, 300)           9078300   \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 41, 128)           384128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 9,462,815\n",
      "Trainable params: 384,515\n",
      "Non-trainable params: 9,078,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "175/175 [==============================] - 18s 100ms/step - loss: 0.4190 - acc: 0.7152 - val_loss: 0.4120 - val_acc: 0.7320\n",
      "Epoch 2/5\n",
      "175/175 [==============================] - 17s 95ms/step - loss: 0.2695 - acc: 0.8476 - val_loss: 0.3459 - val_acc: 0.7907\n",
      "Epoch 3/5\n",
      "175/175 [==============================] - 17s 97ms/step - loss: 0.1806 - acc: 0.9118 - val_loss: 0.3052 - val_acc: 0.8124\n",
      "Epoch 4/5\n",
      "175/175 [==============================] - 17s 96ms/step - loss: 0.1246 - acc: 0.9435 - val_loss: 0.2881 - val_acc: 0.8204\n",
      "Epoch 5/5\n",
      "175/175 [==============================] - 17s 97ms/step - loss: 0.0934 - acc: 0.9570 - val_loss: 0.2862 - val_acc: 0.8204\n"
     ]
    }
   ],
   "source": [
    "#Set number of epochs and batch size\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "#Set test data to evaluate model\n",
    "y_test = data_test[label_names].values\n",
    "X_test = test_cnn_data\n",
    "\n",
    "#Create model\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))\n",
    "\n",
    "#Fit model and evaluate\n",
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "['partido complicado perdido saldremos fe ultimo pierda', 'asi malditas fujuirraataaaaaassss fujimoristasenemigosdelperu', 'cuidence favor distanciamiento fisico tomen distancia', 'numero contagios hoy fallecidos regiones please', 'partido complicado perdido saldremos fe ultimo pierda', 'gobierno incompetente']\n",
      "[0.96882, 0.02309, 0.00809]\n",
      "[0.01396, 0.05144, 0.93459]\n",
      "[0.08014, 0.76291, 0.15694]\n",
      "[0.0328, 0.92384, 0.04336]\n",
      "[0.96882, 0.02309, 0.00809]\n",
      "[0.01748, 0.00121, 0.9813]\n"
     ]
    }
   ],
   "source": [
    "#Test the model with example\n",
    "text = [\n",
    "    \"Partido complicado pero no perdido. Saldremos de esta 💪 Que la fe sea lo ultimo que se pierda 🙌\",\n",
    "    \"@manuval68 @Minsa_Peru @Agencia_Andina @noticias_tvperu @RadioNacionalFM @DiarioElPeruano ASI ES!!!!!!!!!!!!!!!!!!!! MALDITAS FUJUIRRAATAAAAAASSSS!!!!!!!!!!!!!!!! #FujimoristasEnemigosDelPeru!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\",\n",
    "    \"@Minsa_Peru @Agencia_Andina @noticias_tvperu @RadioNacionalFM @DiarioElPeruano Cuidence por favor !!!!!!!!! distanciamiento fisico ! ❤️❤️❤️❤️🙏🤍💪 tomen distancia 🥺\",\n",
    "    \"@rparrawong @Minsa_Peru Tienen el numero de contagios de hoy y fallecidos por regiones please?? 🙁\",\n",
    "    \"Partido complicado pero no perdido. Saldremos de esta 💪 Que la fe sea lo ultimo que se pierda 🙌\",\n",
    "    \"@pcmperu @nlcr5_ @presidenciaperu @PeruPaisDigital @MTC_GobPeru @Minsa_Peru @EsSaludPeru @elcomercio_peru @larepublica_pe @RPPNoticias @canalN_ @exitosape Otra cojudez de este gobierno incompetente\"\n",
    "]\n",
    "text = [preprocess_text(t) for t in text]\n",
    "text = tokenizer.texts_to_sequences(text)\n",
    "print('----------------')\n",
    "print(tokenizer.sequences_to_texts(text))\n",
    "text = pad_sequences(text, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "predictions = model.predict(text)\n",
    "for p in predictions:\n",
    "    p = [round(num,5) for num in p]\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export model\n",
    "import pickle\n",
    "\n",
    "with open('tokenizer2.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\users\\rodrigo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: cnn_model2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"cnn_model2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
